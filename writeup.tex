\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{CS 476/676}
\newcommand\semester{Fall 2013}     % <-- current semester
\newcommand\hwnum{1}                  % <-- homework number
\newcommand\yourname{David Snyder} % <-- your name
\newcommand\login{Hopkins ID: 38C8F8}       
\newcommand\hwdate{Due: October 2nd 2013}           % <-- HW due date

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}


\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ \login\\\course\ --- \semester}
\chead{\textbf{\Large Homework \hwnum}}
\rhead{\hwdate}
\headsep 10pt

\begin{document}

\noindent \emph{Homework Notes:} Each section of the assignment represents roughly equal contribution by both Nely Jimenez and David Snyder. \\
\begin{answer}{1a}

Let $X_{n}$ and $Y_{n}$ represent the first $n$ observations of both the stock values A, B, C and D, 
and the resultant change in the value of the stock Y. The prior at time $n+1$ is given by 

$p(\mathbf{w} \mid \mathbf{X}, \mathbf{Y}, {\sigma}^{2})$

which is proportional to 

\[
N(\mathbf{w} \mid \mathbf{\mu}_{0}, \mathbf{ {\Sigma}}_{0} ) N( \mathbf{Y} \mid \mathbf{{Xw}}, {\sigma}^{2} \mathbf{I}_{n}) = N(\mathbf{w} \mid \mathbf{w}_{n}, \mathbf{\Sigma}_{n})
\]

As described by Murphy, equations (7.55-7.58).\\ \\

After observing $n$ data points, the probability of ${\mathbf{w}}$ at time $n+1$ is given by, 
\begin{eqnarray*}
  p(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid {\mathbf{w}}_{n}, {\mathbf{\Sigma}}_{n}).
\end{eqnarray*}
Where 
\begin{eqnarray*}
  {\mathbf{w}}_{n} = {\mathbf{\Sigma}}_{n}  {{\mathbf{\Sigma}}_{0}}^{-1}  {\mathbf{\mu}}_{0} + \frac{1}{{\sigma}^{2}}  {\mathbf{\Sigma}}_{n} {{\mathbf{X}}_{N}}^{T} \mathbf{y} \\
  {\mathbf{\Sigma}}_{n} = {\sigma}^{2} ({\sigma}^{2} {{\mathbf{\Sigma}}_{0}}^{-1} + {\mathbf{X}}^{T} \mathbf{X} )^{-1}
\end{eqnarray*}

\end{answer}

\begin{answer}{1b}

Given $\textbf{X}_{n}$, $\textbf{x}_{{n+1}}$ and $\textbf{y}_{n}$ and the prior in 1a, the full posterior predictive distribution
is given by the following equation (see Murphy equation 7.60),
\[
p(y_{{n+1}} \mid \textbf{x}_{{n+1}}, D, {\sigma} ^ {2}) = \int N(y_{{n+1}} \mid \textbf{x}_{{n+1}} ^ {T} \textbf{w}, {\sigma} ^ {2}) N(\textbf{w} \mid \textbf{w}_{n}, {\Sigma}_{n}) d \textbf{w} )
\]
The previous expression is equivalent to 

\[
N(y_{{n+1}} \mid \textbf{w}_{n} ^ {T} \textbf{x}, {\sigma} ^ {2} (\textbf{x}_{{n+1}})),
\]
where ${ \sigma } ^ {2} (\textbf{x}) = {\sigma} ^ {2} + {\textbf{x}} ^ {T} {\textbf{\Sigma}}_{n} \textbf{x}$
as given in equation 7.61 Murphy.

\end{answer}

\end{document}
